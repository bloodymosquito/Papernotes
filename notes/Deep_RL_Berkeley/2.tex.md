# [@](./README.md) Lecture 2: Supervised Learning of Behaviors

**Today**: 
1. define sequential decision problems
2. Imitation learning: superived learning for decision making
3. A bit of theory
4. Case studies of recent deep imitation learning work


## Terminology and Notations

$\pi_\theta(a_t \mid o_t)$

Where: 
- $o_t$ the observation (ex: an image of tiger)
- $a_t$ the action (ex: labelling 'tiger', or run away, or a continuous action)
- $\pi_\theta$ the *policy*. 

**Remark**: in this setting, we miss the fact that the action you take affects the next observation you see.

**State vs. Observation**

The state fully describes what happens in the world, whereas the observation is what is "available":
- $\pi_\theta(a_t \mid s_t)$ = fully-observed situation
- $\pi_\theta(a_t \mid o_t)$ = partially-observed situation

Markov property: future independent of the past given the present. WARNING: valid for states, not observations! (ex: car hiding the cheetah) 


## Behavioral cloning

**Principle**: Supervised learning on pairs $(o_t, a_t)$ given by human demonstrations.

**Butterfly effect**: Small mistakes become huge mistakes because during the trajectory, because small errors compound. Small generalization error at first puts you in a unknown situation, and you make a bigger generalization error, and so on.

**Improvements**: 
- Data augmentation, for stability: addtional cameras, ask human to make small errors, etc.
- DAgger (Dataset Aggregation): add on-policy data, labeled by human expert, then retrain, etc.

Problems with trying to directly fit the expert: 
1. Non-markovian behavior: human demonstrators are not markovian ($\pi(a_t \mid o_1, ..., o_t)$). We may want to include history, but beware of *causal confusion* (brake light) in IL.
2. Multimodal behavior: a solution is to output a mixture of gaussians (but up to $n$ modes), use latent variable models or autoregressive discretization.


Imitation learning (at least behavioral cloning) is intrinsically limited and problematic: 

- Humans need to provide data, which is typically finite, but deep learning works best when data is plentiful.
- Humans are not good at providing some kinds of actions (e.g. when the task is overwhelming for a human).
- We don't meet an interesting feature of humans: being able to learn autonomously and to keep on improving with their self experience. 

That's why we would like to learn **without imitation**.

**Idea**: "what you really want is not to imitate somebody's actions, but to *minimize the expected number of times you're eaten by the tiger*" XD.


## Modifying terminology

$\max_\theta E_{s_{1:T}, a_{1:T}} \Big[ \sum_t r(s_t, a_t) \Big] $


## Why Behavioral Cloning can be problematic

Tight rope walker.

### Basic derivation

Setting: 
- Length of the trajectory: $T$. 
- Cost: $c(s, a) := \mathbb{1}(a \ne \pi^*(s))$ (doing a thing that the human would not have done).
- **Assumption**: $\pi_\theta(a \ne \pi^*(s) \mid s) \le \varepsilon, \forall s \in \mathcal{D}_{train}$ ((learning memorized human actions to a some degree).

> Let's construct the worst possible situation: 
>
> &rarr; The *imitation learning* tight rope walker.
>
> Imitation learning, since he is not afraid of falling because of a bad reward, but because he will end up in a state on which he has no training info (the human walker never fell).

$E \Big[\sum_t c(s_t, a_t) \Big] \le \varepsilon T + (1 - \varepsilon) ( \varepsilon (T-1) - (1 - \varepsilon) (...))$

Thus: $O(\varepsilon T^2)$

Bad news: error scales quadratically with the length of the trajectory.

**BUT**: the assumption is not very realistic, since we can expect some generalization from the learning!

---

### More general derivation (used later to analyze Policy Gradient methods)

Let's take generalization into account in our new setting: 
- Same cost
- **Assumption**: $\pi_\theta(a \ne \pi^*(s) \mid s) \le \varepsilon, \forall s \sim p_{train}(s)$

>**Simple analysis**: with DAgger, $p_{train}(s) \rightarrow p_\theta(s)$
>
>Therefore, all the encountered states with the policy are somehow in the training data. So, at each step, we have a $\varepsilon$ probability of error.
>
>So: $E \Big[\sum_t c(s_t, a_t) \Big] \le \varepsilon T$ (best that we can do)

Now, for behavioral cloning, no such assumption (we have $p_{train} \ne p_\theta$).

$p_\theta(s_t) = (1 - \varepsilon)^t p_{train}(s_t) + (1- (1 - \varepsilon)^t) p_{mistake}(s_t)$

> Notation (*total variation divergence* between two distributions, here $p_\theta$ and $p_{train}$): 
> $ TVD(p_\theta, p_{train}) := \sum_{s_t \in S} \mid p_\theta(s_t) - p_{train}(s_t) \mid$
> **Remark**: at most 2 (disjoint supports), at least zero (identical distributions).

We can express the difference between the training and the policy distributions, at instant t: 

$ TVD(p_\theta, p_{train})_t = \big(1 - (1 - \varepsilon)^t \big) \sum_{s_t} \mid p_{train}(s_t) - p_{mistake}(s_t) \mid \le  2 \big(1 - (1 - \varepsilon)^t \big)  \le 2 \varepsilon t$

For the second-to-last inequality, we used the fact that TVD is less than 2.

For the last inequality, we used: $(1 - \varepsilon)^t \ge 1 - \varepsilon t, \forall \varepsilon \in [0, 1]$

Now, let's get a bound for our cost:

$E \Big[ \sum_t c_t \Big] = \sum_t E_{p_\theta(s_t)} \Big[ c(s_t) \Big] = \sum_t \sum_{s_t} p_\theta(s_t) c(s_t) = \sum_t \sum_{s_t} p_{train}(s_t) c(s_t) + \big( p_\theta(s_t) - p_{train}(s_t) \big) c(s_t) $

So:
 
$E \Big[ \sum_t c_t \Big] \le \sum_t \sum_{s_t} p_{train}(s_t) c(s_t) + \mid p_\theta(s_t) - p_{train}(s_t) \mid c_{max} = \sum_t E_{p_{train}} \Big[ c(s_t) \Big] + TVD(p_\theta, p_{train})_t $

(since $c_max = 1$). 

The first term is the expectation of error under the training distribution, so $\varepsilon$ by definition.

Therefore:

$E \Big[ \sum_t c_t \Big] \le \sum_t \varepsilon + 2 \varepsilon t = \varepsilon T  + 2 \varepsilon \frac{T(T+1)}{2}$

**&rarr; Again, we find a $O(\varepsilon T^2)$ bound.**
